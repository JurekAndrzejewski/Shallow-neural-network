Implement a shallow (up to 5 layers) fully connected neural network. The network should be
based on the neuron implemented in the task #2 and use the data generated by the task #1 as
its input and yield the predicted class membership at its output. According to theory, any data
classification problem should be solvable using a three-layered network, thus at the minimum
the implemented solution should support the evaluation and training of such a network. The
output should be presented in the form of two values describing the confidence that the network
belongs to each class. The network should consist of a two-neuron input layer, a two-neuron
output layer, and at least one hidden layer, the remaining values of width and depth of the
network should be configurable. The neurons should use the logistic activation function, though
the implementation may be extended by adding more options. The network should be trained
using the backpropagation formula:

As was the case in task 2, the GUI should present the decision boundary for the network through
colouring the corresponding parts of the plot.